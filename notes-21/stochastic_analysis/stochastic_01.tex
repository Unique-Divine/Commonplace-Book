\chapter{Probability and Stochastic Processes}

\section{Motivation for learning about stochasic processes}

The modelling of stochastic processes is one of the main applications of machine learning. A few examples:
\begin{itemize}
	\item Poisson processes: For dealing with waiting times and queues.
	\item Random walk and Brownian motion processes: Used in algorithmic trading
	\item Markov decision processes: Commonly used in computational biology and reinforcement learning. HMMs are generally useful for understanding sequences and have applications for both writing and speech processing tasks.  
	\item Auto-regressive and moving average processes: For time series analysis. ARIMA models. 
\end{itemize}



% ---------------------------------------
\section{Probability Theory I [Lec. 0, Jan 11]}
% ---------------------------------------

\subsection{Course Overview}

The course will be non-traditional. It's not going to be your typical course found in a statistics or pure math department. What we'll do is present tools from stochastic analysis that are often useful in research and in industry for modeling physical systems.

The usual treatment of this subject is to go over some theoretical results and then talk about a few applications in finance. What we want to look at is the applications of this field in applied math. For instance, elliptic partial differential eqs, monte carlo methods, etc. This will cover the first few chapters of the textbook.

The goal is to gain an overall intuition for the subject, so we're not going to talk about all of the technical details. This doesn't mean we'll have fallacies in all of our derivations. It just means that we won't prove everything so that we can save time. We'll mostly look at the big picture and the connection to different things. We'll talk about why certain abstract things are actually useful.

Consequently, you'll see a lot of jumps. We'll also review elementary knowledge in this area and computing.

The first few homeworks will be a recap of some probability theory that we'll use. Limiting theorems, random variables, and distributions. The rest of the homework is mostly on projects. We will sometimes have simple derivations, schemes, code implementations of course concepts.


Today won't even be a review. We'll just mention what knowledge you will need.

\subsection{Probability Theory Review}

\begin{enumerate}
	\item probability spaces: ($\Omega, F, \pr$) = sample space, $\sigma$-algebra, probability measure

	A sigma alebra has a few properties (in first few chapters of textbook). "countable union"
	\begin{itemize}
		\item $\phi \in F$
		\item $A\in F \implies A^c\in F$
		\item $\{ A_i\}_{i=1}^\infty \in F \implies \cup A_i \in F$
	\end{itemize}

	A probability measure is a function that maps between 0 and 1. $\pr: f\to [0,1]$.
	\begin{itemize}
		\item $E_1 \subseteq E_2 \implies \pr(E_1) \leq \pr(E_2)$
		\item Boole's Inequality: $\pr(\bigcup\limits_{i=1}^\infty E_1)  \leq \sum\limits_i \pr(E_i)$ 
		\item Inclusion-Exclusion: $\pr(E_1\cup E_2) = \pr(E_1) + \pr(E_2) - \pr(E_1 \cap E_2)$
	\end{itemize}
	
	\item  Conditional Probability 
	\begin{itemize}
		\item independence def.
		\item conditional prob. def.,  Bayes' Thm
		\item Law of total prob.: Let $\{E_i\}$ be pairwise disjoint s.t. $\bigcup_i E_i = \Omega$ and $\pr(E_i) > 0$. Then, $\pr(E) = \sum_i \pr(E|E_i) \pr(E_i) = \sum_i \pr(E\cap E_i)$. 
	\end{itemize}
	
	\item \textbf{Random Variables}.
	
	A random variable is a measurable real-valued function, $X(\omega): \Omega\to \mathbb{R}$.

	Measurable $\equiv \; \forall x, \; \{\omega | X(\omega) \leq x\} \subset F $ 

	Distribution: The probability distribution function, $\pr(X\leq x) = F_X(x)$.  
	If \[\exists f_X(x)\text{ s.t. }F_X(x) = \int\limits_{-\infty}^x f_X(t)dt, \;\; \forall x,\] 
	then $f_X$ is a PDF and  $F_X$ is a CDF.

	Expectation: $\expec[X] = \int\limits_{\Omega} x f_X(t)dt$. Sometimes we write this more simply as 
	\[\expec[X] = \int\limits_\Omega X d\pr = \int\limits_{-\infty}^\infty x f_X(x) dx.\] 

	Thm: $X\geq 0 \implies \expec[X] \geq 0$. 

	\[ \expec[a + bX] = a + b\expec[X]	 \]

	\[ \expec[aX + bY] = a\expec[X] + b\expec[Y] \]

	\[ \{X_i\}_i\text{ independent }\implies \expec[\prod_i X_i] = \prod_i \expec[X_i] \]
\end{enumerate}

Variance: Var($X$) = $\expec[(X - \expec[X])^2]$

\[ 
	\text{Cov}(X, Y) = \expec[(X - \expec[X])(Y - \expec[Y])]
\]

\[
	\text{Corr}(X, Y) = \frac{ \text{Cov}(X, Y)}{ \sqrt{\sigma_X^2 \sigma_Y^2} }	
\]

Also note that $\sigma_X^2 = \expec[X^2] - \expec[X]^2$

\[ \Var(a + bX) = b^2 \Var(X) \]

\[ \Var(X + Y) = \Var(X) + \Var(Y) + 2\text{Cov}(X, Y) \]


% ---------------------------------------
\section{Probability Theory II [Lec. 1, Jan 13]}
% ---------------------------------------

Moment inequalities

Thm Markov's Ineq

If $\expec[X] <\infty$, then 
\[ \pr(|X| \geq a) \leq \frac{\expec[|X|]}{a}, \;\; a\geq 0 . \]

\begin{proof}
	
\end{proof}

theorem:
$\phi$ is monotone increasing
\[ \pr(|x| \geq a)  = \frac{\expec[ \phi(|x|)]}{ \phi(a)}.\]

Take $\phi(x) = x^2$.
\[
\begin{aligned}
\implies Y = |x - \expec[x]| \\
\implies \pr(|x - \expec[x]| \geq a) \leq \frac{ \expec(|x - \expec[x]|^2) }{ a^2 }
\end{aligned}
\]




Why si this useful? It means that if you knwo how to control the variance, then you know how to control the probability. In the more general case, $\phi$ might be the third (or other higher order) moments.

\begin{proof}
$\pr(|x| \geq a) = \pr( \phi(|x|) \geq \phi(a) )$. Then Markov's Inequality.
\end{proof}


Chebyshev Inequality is one of the fundamental inequalities you should have seen. You should also be familiar with moment generating functions.

Another one you should know: Jensen's Inequality.

Jensen's Inequality (Theorem): Let $f(x)$ be convex. Then, $\expec[f(x)] \geq f(\expec[x])$.

Another one that is important is cauchy-schwarz .

Cauchy Schwarz Inequality (Theorem): Suppose you have two random variables, $X$ and $Y$ s.t. $\expec[X^2] < \infty$ and $\expec[Y^2] < \infty$.
\[ \implies \expec[XY]^2 \leq \expec[X^2] \expec[Y^2]. \]

\begin{proof}
$\forall a, b \in \mathbb{R}$ define $Z = aX - bY$. You can then show that
\[\begin{aligned}
\expec[Z^2] &= \expec[(aX - bY)^2] = a^2 \expec[X^2] - 2ab \expec[XY] + b^2 \expec[Y^2] \geq 0 . \\
&\implies \;\; (2b \expec[XY])^2 - 4 \expec[X^2] \cdot b^2 \expec[Y^2] \leq 0 \\
&\implies \;\; (\expec[XY])^2 \leq \expec[X^2] \expec[Y^2]
\end{aligned}\]
\end{proof}


\subsection*{7. Characteristic Function}
We're concerned with the characteristic fn of random variables, function spaces, or distributions. It's all the same stuff. It doesn't matter.

Let $X$ be a R.V. on ($\Omega, F, \pr$). Given $\phi(t) := \expec[e^{itX}] \forall t\in \mathbb{R}$.

\begin{note}
This is called a fourier transform. It looks similar to the moment generating function,
$M_X(t) \equiv \expec[e^{tX}], \;\; t\in\mathbb{R}$.
\end{note}


$\phi(t) = \int_{-\infty}^{\infty} e^{itx} f(x) dx, \;\;\; f(x)dx := dF(x)$

Example 1.

$X\sim \text{Unif}(a, b)$.

$\phi_X(t) = \expec[e^{itX}] = \int\limits_a^b$


Example 2.

$X \sim \mathcal{N}(0, 1)$.

$\phi_X(t) = \expec[e^{itX}] = \int\limits_{-\infty}^\infty e^{itx} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx$

$= e^{\frac{-1}{2}t^2}$


\textbf{THm}:
\begin{gather}
\phi(0) = 1 \\
|\phi(t)| \leq 1, \forall t\in \mathbb{R}
\end{gather}
\begin{proof}
\[\begin{aligned}
|\phi(t)| = |\int e^{itX} dF| \leq int |e^{itX}| dF \leq 1.
\end{aligned}\]
\end{proof}


\textbf{Thm}: Let $\{ x_k\}_{k=1}^n$ be independent. Let $z = \sum_k x_k$.

How do I find a distribution of $z$? We do a convolution.

$\phi_z(t) = \phi_{x_1} + \ldots + \phi_{x_n}$.  By performing an inverse Fourier transform of the RHS, I can find the characteristic function. The "convolution" will give me the distribution. "We don't need to prove this."


\subsubsection*{Thm} Let $x$ (from above) be s.t. $\expec[x^n] < \infty$. Then, $\forall k\leq n,$ $\phi^{(k)}(t) = i^k \int x^k e^{itx} dF(x)$

$\implies \phi^{(k)}(0) = i^k \int x^k dF(x)  = i^k \expec[x^k]$

$\implies \expec[x^k] = i^{-k} \phi^{(k)}(0)$. The superscript notation denotes the $k$th derivative.


\subsection{Law of Large Numbers (LLN)}

\subsubsection*{Bernoulli's Weak LLN (Thm)}

Why is it weak? We'll explore this. It has to do with weak convergence.

This theorem involves looking at a sequence of i.i.d. random variables.  Let $\{ x_n\}_{n\in N}$ be  a seq of i.i.d. R.V.s with $\sigma^2 = \text{Var}(x_n)$

Define $S_n = \sum_{k=1}^n x_k$. Then, $\dfrac{S_n}{n} \xrightarrow[]{\pr} \mu := \expec[x_n]$ as $n\to\infty$.

\begin{mybox}{green}{definition of "convergence in probability"}
\[\forall \epsilon > 0 , \;\;\; \lim\limits_{n\to\infty} \pr(|\frac{S_n}{n} - \mu| \geq \epsilon) = 0 \]
\end{mybox}

\begin{proof}
By the Chebyshev Ineq., $\pr\left( |\frac{S_n}{n} - \mu| \geq \epsilon \right)
	\leq \frac{\expec[(\frac{S_n}{n} - \mu)^2] }{ \epsilon^2 }
= \frac{  \frac{1}{n^2} \expec[(S_n - n\mu)^2] }{ \epsilon^2 } $

$= \frac{text{Var} (S_n) }{ n^2\epsilon^2 } = \frac{n\sigma^2}{ n^2\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0$.
\end{proof}

\subsubsection*{Kinchtin Weak LLN (Thm):} Let $\{ X_n\}$ be i.i.d. be R.V. with $\mu := \expec[X_n] < \infty$. Then, $\forall \epsilon,$ $n\to \infty \implies \pr(|\frac{S_n}{n} - \mu| \geq \epsilon) \to 0$.



\begin{note}
There is a final project, and you'll have more information about it throughout the next few weeks.

A homework will come out next week on Monday. There's a link on courseworks to the office hours and the syllabus section.
\end{note}

\section{Convergence of RVs I [Lec. 2, Jan 20]}

\subsection{Probability Theory III}

\subsubsection*{Thm. Kolmogorow Strong Law of Large Numbers}

Let $\{ X_n\}$ be i.i.d. RVs with $\expec[X_n] = \mu < \infty$. Then, 
\[ 
	\pr \left( \lim\limits_{n\to\infty} \dfrac{S_n}{n} = \mu \right) = 1.
\]

In real analysis, when do we say that a sequence of numbers converges? 
\[ \forall \epsilon > 0 , \exists N \text{ s. t. } |x_n - x| \leq \epsilon \;\; \forall n\geq N. \]

\subsubsection*{Central Limit Theorem} 

If we take a random sample and have convergence, how fast will we see convergence? This is given by the Central Limit Theorem. 

Let $\{X_n\}$ be i.i.d. with 
\[\begin{aligned}
	\expec[X_n] &= \mu, \\
	\Var(X_n) &= \sigma^2, \\
	S_n &= \left(\sum\limits_{k=1}^n \frac{(X_k - \mu)}{n} \right) \frac{\sqrt{n}}{\sigma}
		= \frac{1}{\sqrt{n}} \sum\limits_{k=1}^n \frac{(X_k - \mu)}{\sigma}.
\end{aligned}\]

Then, the sum will be a Guassian RV. 
\[ \lim\limits_{n\to \infty} \pr(S_n \leq x)  = \Psi(X),\] where $\Psi(X)$ is the CDF of $\mathcal{N}(0, 1)$. The speed of convergence is $\frac{1}{\sqrt{n}}$. 

\begin{proof}
	Calculate the characteristic function. 
	\[\begin{aligned}
		\phi_{S_n}(t) &= \expec[e^{it}S_n] 
			= \expec\left[\prod\limits_{k=1}^n 
				\exp\left(
					\frac{it}{\sqrt{n}} \left(
						\frac{X_n - \mu}{\sigma}
						\right) 
					\right) 
				\right] \\ 
		&= \prod\limits_{k=1}^n \phi\left(\frac{t}{\sqrt{n}\sigma}\right)
		= \phi^n \left(\frac{t}{\sqrt{n}\sigma}\right)
	\end{aligned}
	\]

	\begin{gather*}
		\because \phi'(t) = \expec[i(X_k - N) e^{it()}], \therefore \phi(0) = 1. \\
		\phi'(0) = 0 \\
		\phi''(0) = -1 
	\end{gather*}

	Taylor expand
	\begin{align*}
		= (1 - \frac{t^2}{2n\sigma^2}  + O(n^2)) \\
		\phi_{S_n}(t) \xrightarrow[]{n\to\infty} e^{ \frac{ -t^2 }{ 2 } }
	\end{align*}

\end{proof}

The above concludes the recap of what you are assumed to know from a previous probability course. 


\subsection{Convergence}

\paragraph*{Def [Convergence in Law/Distribution]: } $\{X_n\}$ converges to $X$ in law (or in distribution) if 
\[ \lim\limits_{n\to\infty} F_n(X) = F(x), \; \forall \{ x | \text{$x$ is continuous} \}. \]

Notation: $X_n \xrightarrow[]{D} X$ 

This is an extremely weak type of convergene. 

\paragraph*{Def [Convergence in Probability]: } 
$\{X_n\}$ converge in probability to ... if 
\[ \forall \epsilon > 0, \; \lim\limits_{n\to\infty} \pr\left( |X_n - X| >\epsilon \right).  \]

Notation: $X_n \xrightarrow[]{\pr} X$. 

\paragraph*{Almost Sure Convergence (Def): } This is also called convergence w/ prob 1. $\{X_n\}$ converges to $X$ almost surely if 
\[ \pr (\lim\limits_{n\to\infty} X_n = X) = 1 .\]
For each realization, you draw a sequence. Very strong. This is the convergence you see in terms of numbers. Notation: $X_n \xrightarrow[]{\text{a.s.}} X$. There's also something called ``sure convergence'', but we won't worry about it. 

\paragraph*{Convergence in $\ell^p$ norm (Def): } AKA convergence in mean. $\{ X_n \}$ converges to $X$ in the $\ell^p$ norm if 
\[ 
	\lim\limits_{n\to\infty} \expec\left[ |X_n - X|^p \right] = 0.	
\]
For this definition to make sense, we require $\expec[|X_n|^p] < \infty$. When $p=1$, is it called convergence in mean. $p=2 \implies$ convergence in mean-square. 

\paragraph*{ (Thm): } 
\begin{itemize}
	\item $X_n \xrightarrow[]{\text{a.s.}} X \implies X_n \xrightarrow[]{\pr} X$

	and $\implies X_n \xrightarrow[]{D} X$. 
	\item $X_n \xrightarrow[]{\ell^p} X \implies X_n \xrightarrow[]{\pr} X$. 
	\item $X_n \xrightarrow[]{\ell^p} X \implies X_n \xrightarrow[]{\ell^q} X, \;\; 1 \leq q \leq p$. 
\end{itemize}


\chapter{Markov Processes }

When elements of a set are classified as being in one of several fixed states that can switch over time, this process is generally called a \textbf{stochastic process}. The swich between states in a stochastic process is described by a probability that, in general, depends on the current and previous states and the time in question \cite{friedberg2013linear}. 
\begin{itemize}
	\item Ex.: An American voter's preference of political party could be the state. Voting cycles could be modeled as a stochastic process based on these preferences that change with time. 
\end{itemize}

Stochastic prcoesses, i.e. random functions of time, are defined on a set, called the \textbf{index set}. The index set can be discrete or continuous. Markov chains have a discrete index set, while Poisson and diffusion process have continuous ones. 

General stochastic processes are too broad of a class of objects to discuss in much detail, so we'll have to study specific cases of them instead. The specific case that will be the focus of this chapter is the Markov process.   

If the probability to switch between two states of a stochastic process depends only on the two states in question (and not on the time, earlier states, or other factors), then this stochastic process is called a \textbf{Markov process}. To go a step further, if the number of possible states in the Markov process is finite, then the process is called a \textbf{Markov chain} \footnote{Notes will based on chapter 3 of Applied Stochastic Analysis by Weinan et al.}\cite{weinan2019applied}.  Said another way, a Markov process is a process in which knowing the present state makes the future state(s) independent of the past. 

\url{http://langvillea.people.cofc.edu/MCapps7.pdf}

\url{https://math.libretexts.org/Bookshelves/Applied_Mathematics/Book%3A_Applied_Finite_Mathematics_(Sekhon_and_Bloom)/10%3A_Markov_Chains/10.02%3A_Applications_of_Markov_Chains}

\section{[Lec. 3, Jan 25]}

\paragraph*{Formal statment of a stochastic process:} 
$X_t(\omega), t\in \bm{T}$. $\bm{T}$ is $\mathbb{R}$ or $\mathbb{N}$. Hence, $X_t(\omega): \Omega \to \mathbb{R}$ or $X_t(\omega): T\to \mathbb{R}$. In the latter case, it is called a trajectory or sample path. Again, this is a very wide set of functions, so we'll restrict our focus to something more specific to build intuition. 

\paragraph*{Markov Chain (Def): }Let $(\Omega, \mathcal{F}, \pr)$ be the probability space. A Markov chain is a sequence of  random variables, $\{X_t\}_{t\in \bm{T}}$, parameterized with index set, $\bm{T}$, with the Markov property. Each $X_t$ takes values in the state space, $S$. 

\subsection{Discrete time finite Markov chains}

Discrete in time means the parameterization is on a discrete set. Said another way, any countable set can be mapped onto it in a one-to-one manner. 

\paragraph*{Markov chain (Def): }$\{X_n\}_{n\in\mathbb{N}}$ is a Markov chain if 
\[
	\pr( X_{n+1} = x_{n+1}| \{X_k = x_k\}_{k=1}^n) =  \pr ( X_{n+1} = x_{n+1} | X_n = x_n ).
\]
Intuitive definition: The next state is only dependent upon the current state. 

\subsubsection*{Ex. 1 - Markov chain: } 
\begin{gather*}
	\zeta_k := \text{ i.i.d. R.V. s.t.}
		\begin{cases}
			1 & \pr = 0.4 \\
			-1 & \pr = 0.6 
		\end{cases} \\
	X_n := \suml_{k=1}^n \zeta_k \text{ is then Markovian.} \\
	\implies X_{n+1} = \suml_{k=1}^{n+1} \zeta_k = X_n + \zeta_{n+1}. \\
	\pr( X_{n + 1} = x_{n + 1}| \{ X_k = x_k\}_{k=1}^n )
		= \pr(  X_{n + 1} = x_{n + 1}| X_n = x_n ) 
	\pr( X_{n+1} = X_n + 1 | X_n = x_n) = 0.4 \\
	\pr( X_{n+1} = X_n - 1 | X_n = x_n) = 0.6 \\	
\end{gather*}

Having both probabilities set to 0.5 is called symmetric random walk. 

\paragraph*{Finite Markov chain (Def): }A Markove chain is finite if its state space is finite. In Ex. 1, the Markov chain is not finite because its state space is infinite and countable. You could make it finite by taking the modulus and condensing the state space. 

\subsubsection*{Ex. 2 - Boolean Stock Market: }
\[
\begin{aligned}
	X_n = 
		\begin{cases}
		1 & \text{bull year} \\
		-1 & \text{bear year} 
		\end{cases}	\\
	\pr(X_n = 1 | X_n = 1) := \\
	\pr(X_n = -1  | X_n = -1 ) := 0.4
\end{aligned}	
\]
TODO: (above)

\subsubsection*{Chapman-Kalmogorov Eq. (Thm): } 
Let $\{ X_n\}$ be a Markov chain starting in state, $X_0 = i$. Assume the state space, $S$, is countable. Then, 
\[ \pr( X_n = j | X_0 = i ) 
	= \suml_{k\in S} \pr( X_n = j | X_m = k ) \pr( X_m = k | X_0 = i ), \;\; \forall 1\leq m \leq n-1. \]

But what does this mean? Suppose you have a countable sequence of events, $\{ E_k \}_{k=1}^\infty$ s.t. $\bigcup_{k=1}^\infty E_k = \Omega$ and $E_k \cap E_{k'} = \phi, \forall k, k'$. And, $\pr(F) = \suml_{k=1}^\infty \pr( F \cap E_k ) $. So, Chapman-Kalmogorov is basically the law of total probability twisted a bit. 

You have a process in which you're jumping from state $i$ to state $j$. This theorem states that the probability of such an setup is the sum of all possible intermediate jumps. 

\begin{proof}
	\begin{align*}
		\pr( X_n = j | X_0 = i) 
			&= \suml_{k\in S} \pr \left( X_n = j \cap X_m = k | X_0 = i \right) \\
			&= \suml_{k\in S} \pr \left( X_n = k | X_m = k \cap X_0 = i \right) 
				\pr (X_m = k | X_0 = i ) \\
			&= \suml_{k\in S} \pr( X_n = j | X_m = k ) \pr( X_m = k | X_0 = i) \tag{Markov assumption}
	\end{align*}
\end{proof}

\paragraph*{Invariant distribution of stationary Markov chains: } Let $S = \{ 1, 2, \ldots, I \}$ be a countable set of states. Don't be alarmed by these integers in $S$. These are just labels for the states similar to how we labeled bear and bull markets 1 and -1 in Ex. 2. With this state, we can define the transition probability. 

\paragraph*{Transition probability (Def): } The transition probability at step $n$ is 
\[ 
	\pr_{kj}^{(n)} = \pr (X_{n+1} = j | X_n = k ) .
\] 
The superscript means ``at step [superscript]". The order of the symbols in the subscript indicates the order of events, so you may see $\pr_{kj}^{(n)} = \pr (X_n = j | X_0 = k)$ to mean the same thing in another text. In our notation, it means ``transition from state $k$ to $j$."  If $\pr_{kj}^{(n)}$ is independent of $n$, we say that the MC is \textbf{stationary}. 

\paragraph*{Stationary transition matrix: } With a stationary transition probability $\pr_{kj}$, a stationary transition matrix can be defined as $P = (P_{kj})_{kj\in S}$. 
Its columns are probability vectors.
\begin{itemize}
	\item $P_{kj} \geq 0\; \forall k, j$. 
	\item $\suml_{j\in S} P_{kj} = 1 \;\; \forall k \in S $.  
\end{itemize}


\section{[Lec. 6, Feb. 3]}

We have some observations, $\{ Y_n \}$,  in the HMM. See Kui Ren's Lecture Notes 01-C.pdf. 

An Emmission matrix of a HMM is $R$ with elements $R_ij = Y_ij$, where $i\in S$ and $j \in O$. $Y_ij := \pr(Y_j = y | X_i = x)$
The parameters for this model are $\theta := (\bm{\mu}_0, \bm{P}, \bm{R})$. The main quantities of interest for the two sequences of random variables $X = (X_1, \ldots, X_n)$, $Y= (Y_1, \ldots, Y_n)$:
\begin{itemize}
	\item $\pr(X | \theta) = \bm{\mu}_{0, X_1} \pr_{X_1X_2} \pr_{X_2X_3} \cdots \pr_{X_{n-1}X_n} = \bm{\mu}_{0, X_1} \prod_{k=1}^{n-1} \pr_{X_k X_{k+1}}$

	\item By the law of total probability, $\pr(Y|X, \theta) = \pr(Y_1| X_1, \theta)\pr(Y_2| X_2, \theta)\cdots \pr(Y_n | X_n, \theta) = \prod_{k=1}^n \pr(Y_k | X_k, \theta)$

	\item $\pr(X, Y | \theta) = \pr...$
\end{itemize} 

\subsection{Parameter Estimation in HMMs}: We'd like to know the mapping of $Y \to \bm{\theta}$ in order to estimate the parameter $bm{\theta}$.
\begin{quest}
	\item How do you find the parameter $\bm{\theta}$ in a HMM?
	\begin{ans}
	We know $\pr(\bm{Y}|\bm{\theta})$, which is the parameter-to-observation map, and can use maximum likelihood methods to find the parameter, $\bm{\theta}$, that most likely has the generated observation, $\bm{Y}$. In other words, we solve for $\pr(\bm{\theta} | \bm{Y}) = \dfrac{\pr(\bm{Y} | \bm{\theta}) \pr(\bm{\theta}) }{ \pr(\bm{Y}) } $.  
	\end{ans}
	
	\item How does a maximum likelihood method work?
	\begin{ans}
	Method 1: The parameter that has the largest chance to generate the observation is $ \bm{\theta}^* = \arg\max_{\bm{theta}} \pr (\bm{Y} | \bm{\theta})$. It is then called the maxmimum likelihood estimator.  

	Method 2: Maximum a posteriori (MAP) estimation: A Bayesian approach. 
	\end{ans}
\end{quest}

\subsection{Continuous Time Finite Markov Chain}

Lecture Notes 01-D.pdf 

Let $\{ X_t \}_{t\in\mathbb{R}^+}$ be a continuous time stochastic process. In order for $\{ X_t \}$ to be a Markov process, it must satisfy the Markov property:
\[
	\pr(X_{t + s} = x_{t + s} |  \{ X_{t'} \}) 
	= \pr(X_{t + s}  = x_{t + s} | X_s = x_s),
\] where $t' \in [0, s]$. 


\begin{itemize}
	\item $\{ X_t \}$ is right-continuous. $\therefore \quad \lim\limits_{h \to 0_+} X_{t + h} = X_t$. 
	\item $\{X_t\}$ has a finite state space, $S = \{1, 2, \cdots, I\}$. 
	\item Transition probability: $p_{jk} (t) = \pr (X_{t + s} = k | X_s = j)$
	\item Assumed stationarity, or homogeneity in time. This means that the transition probability is independent of $s$. 
\end{itemize}

\begin{quest}
	\item This is the continous picture for finite Markov Chains. How do you map to the discrete case?
	\begin{ans}
		The discrete Markov chain is simply the continuous case fixed with $t = 1$ in the transition probability. Said another way, the step size of the "jump" has size 1 for the discrete case rather than size $t$. 
	\end{ans}
\end{quest}

Notes from homework 2:
\begin{itemize}
  \item \href{http://web.math.ku.dk/noter/filer/vidsand12.pdf}{problem 1 on  bottom of page 6}
  \item \href{https://en.wikipedia.org/wiki/Proofs_of_convergence_of_random_variables#:~:text=Convergence%20almost%20surely%20implies%20convergence%20in%20probability,-Proof%3A%20If%20%7BX&text=This%20means%20that%20A%E2%88%9E,(A%E2%88%9E)%20%3D%200.&text=which%20by%20definition%20means%20that%20Xn%20converges%20in%20probability%20to%20X.}{Convergence proofs}
  \item \href{https://youtu.be/AH5jnR3RxJU}{Convergence in prob. of the sum of two RVs}
  \item \href{http://www.ma.huji.ac.il/~razk/Teaching/LectureNotes/Probability/Chapter7.pdf}{More convergence proofs}
  \item \href{http://www.stat.cmu.edu/~larry/=stat325.01/chapter5.pdf}{Even more convergence proofs- Q4 is on pages 3-4.}
\end{itemize}


