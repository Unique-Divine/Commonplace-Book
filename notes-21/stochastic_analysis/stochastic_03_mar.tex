\chapter{March - Stochastic Analysis }

\section{Homework 4}

\begin{definition}{Covariance of r.v.s}{}
	\begin{gather*}
		\text{Cov}(X, Y) = \E[(X - \E(X))(Y - \E[Y])] \\
		\Var(X) = \text{Cov}(X, X) = \E[(X - \E[X])^2] = \E[X^2] - \E[X]^2
	\end{gather*}
\end{definition}

\cloze Two r.v.'s $X$ and $Y$ are called uncorrelated if $\text{Cov}(X, Y) = 0$. 

\subsection{Standard Wiener process (Q1)}
Let $\{ W_t\}_{t\geq 0}$ be a standard Wiener processes. Evaluate the following quantities. 

\begin{enumerate}[label=(\roman*)] 
\item % 1 i 
$\E[W_t^4]$.

\item % 1 ii
$\E[(W_t - W_s + W_z)^2], \quad t, s, z \in [0, 1].$
\end{enumerate} % end Q1

\subsubsection*{Q1 i}
Here, I'm tasked with finding the expectation of a Brownian motion. Expectiations are defined by $\E(f(X)) = \int f(s) \rho_X(s) \d s$, where $\rho_X$ is the PDF of r.v. $X$. 

So, if we wanted to compute $\E[W_t^4]$, this would be 
\begin{gather*}
	\E[f(W_t)] = \int f(w) \rho_{W_t}(w) \d w	 \\
	\E[W_t^4] = \int w^4 \rho_{W_t}(w) \d w \tag{1} 
\end{gather*}
What then is $\rho_{W_t}$? Well, we know that $\{W_t\}$ is a standard Brownian motion. Thus by the normality of increment property for a Brownian motion, $W_t - W_s \sim \mathcal{N}(0, t-s)$ for all $t >s \geq 0$. Since the motion is standard, $W_0 = 0$, which implies that $W_t - W_0  = W_t \sim \mathcal{N}(0, t)$ for all $t > 0$. 
\[ W_t \sim\mathcal{N}(0, t) \iff \rho_{W_t}(w) = \frac{1}{\sqrt{2\pi t}} \exp\left(\frac{-w^2}{2t} \right) \tag{2} \]

Part (i) can be completed by computing the integral in Eq. 1 using the PDF (Eq. 2).

\subsubsection*{Q1 ii}

Computing $\E[(W_t - W_s + W_z)^2], \quad t, s, z \in [0, 1]$ isn't as straightforward. 
\begin{quest}
	\item Are the $W_s$ and $W_z$ terms still standard, and how could I tell?
	\begin{ans}
		Yes, defining $\{W_t\}_{t\geq 0}$ as a standard Wiener process with $t\in[0, 1]$ implies that $W_s$ and $W_z$ would just denote  $\eval{W_t}_{t=s}$ and $\eval{W_t}_{t=z}$, respectively (as long as $s, z \in[0, 1]$). 
	\end{ans}
	\item Derive the expectation of a standard Wiener process. 
	\begin{ans}
		These follow directly from the definitions. If $\{B_t\}$ is a standard B.M. , then $B_0 = 0$ and $B_t- B_s \sim \mathcal{N}(0, t-s)$ for $0\leq s <t\in[0, 1]$. Thus, it's clear from $\E[B_t - B_s] = 0$ that $\E B_t = \E[B_t - B_0] = 0$ too.  
	\end{ans}
	\item What is the expecation of a nonstandard Wiener process?
	\begin{ans}
		TODO 
	\end{ans}
\end{quest}

\subsection{Brownian Bridge (Q2)}

\begin{definition}{Brownian bridge}{}
	A Brownian bridge $\{X_t\}$ is a Gaussian stochastic process s.t. $X_t \equiv B_t - tB_1$, where $\{B_t\}_{t\geq 0}$ is a standard Brownian motion.  
\end{definition}

\begin{fact}{Brownian bridge covariance}{}
	Let $\{X_t\}_{t\geq 0}$ be a Brownian bridge defined by $X_t:= W_t - tW_1$.
	\begin{align*}
	K(s,t) = \Cov(X_t, X_s) 
		&= \Cov(W_t - tW_1, W_s - sW_1) \\
		&= \Cov(W_t, W_s - sW_1) -t\Cov(W_1, W_s - s W_1) \\
		&= \Cov(W_t, W_s) -s\Cov(W_t, W_1) 
			-t\Cov(W_1, W_s) + st\Cov(W_1, W_1) \\
		&= \min(s,t) - s\min(t, 1) -t\min(1, s) + st\min(1, 1) \\
		&= \min(s, t) - st - st + st \\
	\therefore\quad& \boxed{ K(s, t) = \min(s, t) - st }.
	\end{align*}
	\end{fact}
	
	\begin{fact}{Brownian bridge expectation}{}
	Let $\{X_t\}_{t\geq 0}$ be a Brownian bridge defined by $X_t:= W_t - tW_1$.
	\begin{align*}
	m(t) = \E X_t 
		&= \E[W_t - tW_1] \\
		&= \E W_t - t\E W_1 , \quad W_t\sim\mathcal{N}(0, t), 
			W_1 \sim \mathcal{N}(0, 1)\\
	\therefore\quad & \boxed{ m(t) =  0 } \\
	\end{align*}
	\end{fact}


% Q2 Solution
\begin{enumerate}[label=(\roman*)] 
\item % 2 i
$\{X_t\}_{t\geq 0}$ with $X_t:= W_t - tW_1$ is a Brownian bridge.

\begin{mybox}{cyan}{}
\begin{align*}
m(t) &\equiv \E X_t  \\
	&= \E[W_t - tW_1] \\
	&= \E W_t - t\E W_1 , \quad W_t\sim\mathcal{N}(0, t), 
		W_1 \sim \mathcal{N}(0, 1)\\
\therefore\quad & \boxed{ m(t) =  0 } \\
K(s,t) &\equiv \Cov(X_t, X_s). \\
	&= \Cov(W_t - tW_1, W_s - sW_1) \\
	&= \Cov(W_t, W_s - sW_1) -t\Cov(W_1, W_s - s W_1) \\
	&= \Cov(W_t, W_s) -s\Cov(W_t, W_1) 
		-t\Cov(W_1, W_s) \\ &\quad  + st\Cov(W_1, W_1) \\
	&= \min(s,t) - s\underbrace{\min(t, 1)}_{t} 
		- t \underbrace{\min(1, s)}_{s} + st\min(1, 1) \\
	&= \min(s, t) - st - st + st \\
\therefore\quad& \boxed{ K(s, t) = \min(s, t) - st }.
\end{align*}

$\{X_t\}$ fits the definition for a Brownian bridge. 
\end{mybox} % end 2 i

\item % 2 ii
$\{Y_t\}_{t\geq 0 }$ with $Y_t := (1- t) W_{\frac{t}{1-t}}$ for $0\leq t < 1,$ $Y_1 = 0$ is a Brownian bridge.

\begin{mybox}{cyan}{}
\begin{quest}
	\item Derive the covariance function $K(s,t)$ for $\{Y_t\}$. 
	\begin{ans}
		\begin{align*}
			K(s, t) 
				&\equiv \Cov(Y_s, Y_t) 
					= \Cov((1-t) W_{\frac{t}{1-t}}, (1-s) W_{\frac{s}{1-s}}) \\
				&= (1-t)(1-s)\Cov( W_{\frac{t}{1-t}}, W_{\frac{s}{1-s}} )
		\end{align*}
		Since $\Cov(W_t, W_s) = \min(s, t)$, 
		\[ K(s, t) = (1-t)(1-s)\min\left( \frac{t}{1-t}, \frac{s}{1-s} \right) \]
		Consider the case $s < t $, where $s, t\in [0, 1]$.
		\begin{gather*}
			s < t \iff -s > -t \\
			1 - s > 1 - t \\
			\frac{1}{1 - s} < \frac{1}{1-t} \implies \frac{s}{1 - s} < \frac{t}{1-t}
		\end{gather*}
		Thus, if $s < t$, $\min(\frac{t}{1-t}, \frac{s}{1-s}) = \frac{s}{1-s}$. 
		\begin{align*}
			&\therefore\quad K(s, t) = \begin{cases}
				(1 - t) s &,\quad s < t \\
				(1 - s) t &,\quad s > t    
			\end{cases}  
			= \begin{cases}
				s - st &,\quad s < t \\
				t - st &,\quad s > t 
			\end{cases} \\
			& \boxed{ K(s, t) = \min(s, t) - st } 
		\end{align*}
	\end{ans}
\end{quest}
\end{mybox} % end 2 ii 
\end{enumerate} % end Q2

Prove a stochastic process $\{X_t\}$ is a Brownian bridge. This involves proving the process has mean function $m(t) = 0$ and covariance function $K(s, t) = \min(s, t) - st$ for $s,t\in[0,1]$. 

\begin{quest}
	\item What is a Guassian process? ``standard Brownian bridge is a Gaussian process with continuous paths...''
	\begin{ans}
		\S 5.4 Guassian Processes (E et al., 2020)

		\cloze A stochastic process $\{ X_t \}_{t\geq 0}$ is called a Guassian process if its finite-dimensional distributions $\mu_{ \{t_i\}_{i=1}^k }$ are consistent Gaussian measures for any $0\leq t_1 < t_2 < \cdots < t_k$. 

		\cloze A gaussian process $\{ X_t\}$ is determined once its mean and covariance function, 
		\[
			m(t) = \E X_t \quad\text{ and }\quad 
			K(s, t) = \E\left[ (X_s - m(s))(X_t - m(t)) \right],
		\]
		are specified.

		\cloze It is well known that a Guassian random vector $\bm{X} \in M_{n\times 1}$, where $X_i = \bm{X}_i$ are random variable components of $\bm{X}$, is completely characterized by its first and second moments, 
		\[
			\bm{m} = \E\bm{X} \quad\text{ and }\quad
			\bm{K} = \E\left[ (\bm{X} - \bm{m}) (\bm{X} - \bm{m})^T \right]. 	
		\] In component form, 
		\[ m_i = \E X_i \quad\text{ and }\quad 
			\bm{K}_{ij} = \E \left[ (X_i - m_i) (X_j - m_j) \right].
		\] 

		\cloze Using $\bm{m}$ and $\bm{K}$, one can represent $\bm{X}$ via its characteristic function, 
		\[ \E e^{i\bm{\epsilon}\cdot \bm{X}} = \E e^{i\bm{\epsilon}^T\bm{X}}
			= e^{i\bm{\epsilon}^T \bm{m} 
				- \frac{1}{2} \bm{\epsilon}^T\bm{K} \bm{\epsilon} } .\]
	\end{ans}

	\item What is a characteristic function in general? 
	\begin{ans}
	Source: \href{https://en.wikipedia.org/wiki/Characteristic_function_(probability_theory)}{wikipedia - chacteristic function}

	\cloze In probability theory and statistics, the charactristic function of any real-valued stochatic variable completely defines its probability distribution.

	\cloze If a random variable admits a probability density fn. (PDF), then the characteristic fn. is the Fourier transform of the PDF. 

	\cloze The characteristic function always exists when treated as a function of a real-valued argument, unlike the moment-generating function. 

	\cloze Similar to the cumulative distribution function (CDF), the characteristic fn. provides an alternative way to describe a stochastic variable. 

	\cloze For random variable, $X$, the characteristic function is defined by  $\phi_X(t) \equiv \E e^{itX}$.

	\cloze If a random variable admits a probability density fn. (PDF), then the characteristic function is its dual, which means that each of them is a Fourier transform of the other. 

	\cloze If a rand.var. has a moment-generating fn., $M_X(t)$, then the domain of the characteristic fn. can be extended to the complex plane: $ \phi_X(-it) = M_X(t)$. Note however that the characteristic fn. of a distribution always exists, even when the prob. density fn. and moment-generating fn. do not. 
	\end{ans}

	\item How do we find covariance functions?
	\begin{ans}
		First, let's recall some definitions about variance and covariance. 
		\cloze 
		\begin{gather*}
			\text{Cov}(X, Y) = \E[(X - \E(X))(Y - \E[Y])] \\
			\Var(X) = \text{Cov}(X, X) = \E[(X - \E[X])^2] = \E[X^2] - \E[X]^2
		\end{gather*}

		\cloze Two r.v.'s $X$ and $Y$ are called uncorrelated if $\text{Cov}(X, Y) = 0$. 

		All of these definitions extend to the vector case. 

		\cloze If $\bm{X}\in \mathbb{R}^d$ is a stochastic vector s.t. each component $X_k \in \bm{X}$ is a random variable, then the covariance matrix of $\bm{X}$ is defined as 
		\[ \text{Cov}(\bm{X}) = \E\left[ (\bm{X} - \E\bm{X}) (\bm{X} - \E\bm{X})^T \right]. \]
		This is expectation of the dyadic (outer) matrix product of 
		$\bm{X} - \E\bm{X}$ and itself. 
	\end{ans}
\end{quest}

\subsection{Q3 - Karhunen-Loeve}
Q3: Derive the Karhunen-Loeve expansion for the standard Brownian bridge. \emph{Hint: you will need to find eigenpairs of the operator 
$\mathcal{K}f := \int_0^1 K(s,t) f(s)\d s$ on $L^2([0, 1])$ as what we did for the standard Brownian motion.}

\begin{quest}
	\item Karhunen-Loeve expansion?
	\begin{ans}
		page 112 of E et al., Thm. 5.13
	\end{ans}
\end{quest}

\subsection{Q4 - Generators}
Q4: Find the generator of the standard Brownian bridge. 

generator = infitestimal generator

\subsection{Q5 - Fractional Brownian motion}
Q5: A stochastic process $\{ B_t^H \}_{t\geq 0}$ is called a \textbf{fractional Brownian motion} if it is a Gaussian process with mean $m(t) = 0$ and covariance 
\[ K(s, t) = \frac{1}{2} (t^{2H} + s^{2H} - |t - s|^{2H} ), 
	\quad s, t \in [0, T].\] 
The parameter $H\in(0,1)$ is called the Hurst index. Prove that $\{B_t^H\}$ has the following properties: ... 

\begin{enumerate}
\item % 5 i
(Self-similarity): $B_{\beta t}^H$ has the same distribution as $\beta^H B_t^H$ for any $\beta > 0$;

\item % 5 ii
(Stationary increment): $B_t^H - B_s^H$ has the same distribution as $B_{t-s}^H$ for $0 \leq s < t$.

\item % 5 iii
$\{B_t^H\}_{t\geq 0}$ with $B_0^H = 0$ and $H = \frac{1}{2}$ is a standard Brownian motion.
\end{enumerate} % end Q5

``has the same distribution'' $\to$ Is a good way to go about 






\href{https://en.wikipedia.org/wiki/Probability-generating_function}{Probability-generating function}

\href{https://en.wikipedia.org/wiki/Generating_function}{Generating function}

\subsection{Reading Assignments}
\begin{itemize}
	\item Review \S 6.1-6.7 of E, Li \& Vanden-Ejinden
	\item Review Lecture Notes 02 (D-E)
\end{itemize}

\subsection{References - HW4}
\begin{itemize}
  \item \href{http://web.math.ku.dk/noter/filer/vidsand12.pdf}{problem 1 on  bottom of page 6}
\end{itemize}



\section{Hwk 5}

Q2: 3/16 



\section{Lecture 3-08}

Motivation: applications where you want to model the impact of nooise 


study diffeqs of the form $D_t X_t = b(X_t, t) + \sigma(X_t, t) D_t W_t$, where $D_t$ denotes a derivative operator with respect to $t$, $W_t$ denotes a r.v. that is part of a Wiener process, $\sigma(X_t, t)$ is a drift term. 

Remember that Brownian motion has the property that every interval  is independent of everything else. If you naively think of $D_t W_t$ as $\d W_t$, then clearly you have a noise term.  Can we differentiate $W_t$ with respect to $t$? In general, no. 

High level description: We have a physical system $\dot{X_t} = b(X_t, t)$ plus a noise term $\sigma(X_t, t) D_t W_t$. 

If we can't differentiate the quantity $W_t$, what is meant by the $D_t W_t$ term? What this usually means is that you have an ODE, 
\[ \d X_t = b(X_t, t)\d t + \sigma(X_t, t) \d W_t. \]
This is slightly better because it looks like this might make sense. This might still not be a good way to understand this becuase $\d W_t$ is still questionable. If we were to intergrate, we'd get 
\[ X_t - X_0 = \int_0^t b(X_s, s)\d s + \int_0^t \sigma(X_s, s) \d W_s. \] 

We can't normally compute $\int \sigma(X_s, s) \d W_s$, but we can get it through some summations. This integral form we've written down is the right way to understand the stochastic ODE. 

\begin{example}
	$A$ is a finite sum within the interval  $t\in \{t_0,\ldots, t_N\}$ defined by: 
	\[ \E[A] := \sum_{j=0}^{N-1} \left[ W_{t_j} (W_{t_{j+1}} - W_{t_j}) \right]. \]				
	The interval $W_{t_{j+1}} - W_{t_j}$ is is independeent of $W_{t_j}$ since these are part of a Wiener process.   \textbf{TODO: Explain.} 
	\[\therefore \quad \E A 
		= \sum_{t=t_j}^{t_{N-1}} \E W_t \E (W_{t+1} - W_t)
		= 0 .\]
	Let's have $B$ give a right-side summation. In this case, $W_{t_{j+1}} - W_{t_j}$ is not independent of $W_{t_{j+1}}$ because $W_{t_{j+1}}$ is not in the "past". We do, however, know that the difference $W_{t_{j+1}} - W_{t_j}$ has a Guassian distribution.
\begin{align*}
	\E B 
	&:= \sum_{j=0}^{N-1} 
		\E [W_{t_{j+1}} 
		(W_{t_{j+1}} - W_{t_j}) ] \\
	&= \sum_{j=0}^{N-1} 
	\E [(W_{t_{j+1}} - W_{t_j} 
		+ W_{t_j})(W_{t_{j+1}} - W_{t_j}) ] \\
	&= \sum_{j=0}^{N-1} 
		\E \left[  
			(W_{t_{j+1}} - W_{t_j})^2 + 
			W_{t_j} (W_{t_{j+1}} - W_{t_j} ) 
			\right]   \\ 
	&= \sum_{t = t_0}^{t_{N-1}} \E (W_{t+1} - W_t)^2 
		+ \E W_t \underbrace{ \E (W_{t+1} - W_t) }_{=0} \\
	&= \sum_{t = t_0}^{t_{N-1}} 
		\Var (W_{t+1} - W_t) 
		\tag{ $W_{t+1} - W_t \sim \mathcal{N}(0, t+1-t) 
			= \mathcal{N}(0, 1)$ } \\
	&= \sum_{t = t_0}^{t_{N-1}} 1  \\ 
	&= T = \text{card} (\{ t \}),  \quad \text{the total length of the interval}
\end{align*} 
\end{example}

CLass Q: Why not expand the expecation directly? A: We don't know how to compute it. 
\begin{proof}
	$\E [ W_{t_{j+1}} (W_{t_{j+1}} - W_{t_j}) ] 
		= \E [W_{t_{j+1}}^2 - W_{t_{j+1}}W_{t_j}  ]$. We know 
	$\E W_{t_{j+1}}^2$ because that's just the variance, but we don't know how to compute $W_{t_{j+1}} W_{t_j}$. 
\end{proof}

We know that jump terms such as $W_{t+1} - W_t$ are independent of the history and know how they are distributed, so it is advantageous to look to for them when computing expectations.
\begin{align*}
\E [ W_{t_{j+1}} (W_{t_{j+1}} - W_{t_j}) ] 
	&= \E [W_{t_{j+1}}^2 - W_{t_{j+1}}W_{t_j}  ] \\
	&= \underbrace{ \E W_{t_{j+1}}^2 }_{t_{j+1}}
		- \E \left[  
		\underbrace{ W_{t_{j+1}} W_{t_j} }_{\min(t_{j+1}, t_j) = t_j 
		} \right] \\
	&= t_{j+1} - t_j
\end{align*}

\subsection{Ito's integral}

\[ \int_0^T \sigma(X_t, t) \d W_t  
	\approx \sum_{j=1}^{N-1}
	\sigma(X_{t_j}, t_j) [W_{t_j} - W_{t_j}].
\]

The Ito integral uses the approximation defined by $\E A$ in the previous example. The differences in the Brownian motion are taken between $t_{i+1}$ and $t_i$. 

\begin{definition}{Simple measurable function}{}
For any simple simple measurable fn, \textemdash What are simple fns?
For a measurable fn $f$ with  $w\in \{W_t\}$ and $t\in\{t_0, t_N\}$,  it is simple if
$f(w, t)= \sum_{j=0}^n \beta_j (w) X(t_j, t_{j+1}) $, where 
\[ \int_0^T f(W, t) \d W_t = \sum \beta_j [W_{t_{j+1}} - W_{t_j}] \]
In this definition, the Brownian motion $\{W_t\}$ is independent of $\{beta\}$. 
\end{definition}

\begin{definition}{Integral of L-2 stochastic function}{}
FOr a general measurable function, we use a sequence of simple functions to approximate $f(w, t)$:  We define our integral for the usual $\ell$-2 function by the limit 
\[ \int_0^T f(w, t) \d W_t 
	\equiv \lim_{k\to\infty} \int \phi_k (w, t) \d W_t.  \]
What we're basically saying here is that if you have a fn, you can approximate it as a sequence of simple fns.
\end{definition} 

\begin{theorem}{Ito integral expectation}{}
$\E [\int_0^T f(w, t) \d W_t] = 0$.
\end{theorem}

\begin{proof}
If $f(w, t)$ is simple, then 
\[ \E \int_0^T f(w, t) \d W_t  
	= \E \left[ \sum_i \beta_i (W_{t_{i+1}} - W_{t_i})  \right] 
	\propto \E (W_{t_{i+1}} - W_{t_i})  ) 
	= 0 \]
\end{proof}

\begin{theorem}{Ito isometry}{}
$ \E \left[  \left( \int_0^T f(w, t) \d W_t \right)^2 \right] = \E \left[ \int_0^T  f^2(w, t) \d t \right] ,$ the time integral of the square of $f$. 
\end{theorem}

\begin{proof}
Shorthand notation $\to$ $W_{t_{i+1}} - W_{t_i} := \Delta W_{i}$:
\begin{align*}
\E \left[  \left( \int_0^T f(w, t) \d W_t \right)^2 \right] 
	&= \E \left( \sum_i \beta_i ( W_{t_{i+1}} - W_{t_i} ) \right)^2 \\
	&= \E \sum_{i,k} \beta_i \beta_k \Delta W_i \Delta W_k 
		= \E \left[ \sum_{i=k} ("") + \sum_{i\neq k} ("" ) \right] \\
	&= \E \left[ \sum_i \beta_i^2 (\Delta W_i )^2 
		+ 2 \sum_{i< k} \beta_i \beta_k \Delta W_i \Delta W_k \right]
		\tag{time symmetry} \\  
	&= \sum_i \E\beta_i^2  
		\underbrace{ \E (\Delta W_{t_i})^2 }_{=\Var W_{t_i} = \Delta t_i}
		+ 2 \sum_{i < k} 
			\underbrace{  
				\E [\beta_i \beta_k \Delta W_i \Delta W_k]
			 }_{=0\quad\text{by independence} } \\ 
	&= \sum_i \E \beta_i^2 \cdot \Delta t_i 
	  = \E \sum_i \beta_i^2\Delta t_i \\
	&= \E \left[ \int_0^T  f^2(w, t) \d t \right]
\end{align*}
\end{proof}

With Iso isometry and the definition of the integral of a stochastic function, we have NEXT

Tuesday morning OH now avaialble. 

\subsubsection{Final project questions:}
\begin{quest}
	\item ``Did you assign anything about the final project? Currently, I don't know exactly what I need to do for that."
	\begin{ans}
		A: Given papers to read. See how far you can go and basically what they are. If you want to bring your own project,  that will be great. Otherwise, I have plenty of stuff for you. Stochastic wave equations. Wednesday's office hour is still there. 
	\end{ans}
	
	\item Questin about midterm exam 
	\begin{ans} The midterm will be like this. It's open everything except the internet. Find an open library, open desk, whatever, and to give you some ideas about it, for instance, what we talked about today. If I give you a simple example, I can ask you to calculate a stochastic integral, or tell me what idea you'd have if I told you I wanted to study convergence of a Markov chain with $x$ property. It's not a traditional exam, where you need to know super exact information. It's just some random questions to see if you're okay with what we've talked about. I know this is not clear enough, but that's the plan. 
	\end{ans}

	\item ``Will you release what information about what topics will be on [the midterm] or some practice problems to work on beforehand?''
	\begin{ans}
	A: That's a good idea. Topics: basically everything we talked about and nothing beyond that. I'll give you a sample exam just to get ready and see the format of the problems. 
	\end{ans}

	\item ``Will it be online?'' 
	\begin{ans}
	A: Online. It will be your typical 90 minute window. However, there won't be many of the problem solving type questions that you're used to; there will be \emph{understanding} questions. For example, describe what you know about Brownian motion. 
	\end{ans}

	\item Date info
	\begin{ans}
	The midterm is almost all the way at the end of the semester. 
	\end{ans}
\end{quest}
 
\section{Lecture 3-10}

\href{https://cvn.hosted.panopto.com/Panopto/Pages/Viewer.aspx?id=56294e98-70c7-4526-a119-ace701137c35}{[video]}

Continue discussion on Ito's integrals. If you have a fn $f(w,t)$ and want to understand integrals like $\int_0^T f(w, t) \d W_t$. We partition the domain $\{t\} = \{t_0 , \cdots, t_N\}$. $T = \text{card}\{ t\}$. 
\begin{itemize}
	\item Simple $f = \sum_{j=0}^{N-1} \beta_j X(t_j, t_{j+1})$
	\[ \int_0^T f \d W_t := \sum_{j=0}^{N-1} \beta_j \Delta W_j \]
	\item General $f$. 
\begin{gather*} 
	\exists \{ \underbrace{ \phi_k }_{\text{simple}} \} \to f(w, t) \\
	\iff \quad 
		\int_0^T f(w, t) \d W_t 
		:= \lim_{k\to\infty} \int_0^T \phi_k \d W_t  
\end{gather*}
\end{itemize}

\begin{itemize}
	\item $\E \int f(w, t) \d W_t = 0 $ 
	\item (Ito Isometry):
		$\E \left( \int f(w, t) \d W_t \right)^2 = \E \int f^2(w, t) \d t $
	\item $\int_0^T f\d W_t = \int_0^s f \d W_t + \int_s^T f \d W_t$
	\item $\int_0^T (c_0f + c_1g)\d W_t = c_0 \int f\d W_t + c_1 \int g\d W_t  $ 
	\item Define $Y_t := \int_0^t f(w, s) \d W_s$, where $W_s$ is part of a Wiener process. Then this process has continuous trajectories \emph{almost surely}. 
\end{itemize}

\begin{example}
$\int_0^t W_s \d W_s = ?$
\begin{align*}
&\approx \sum_{j=0}^{N-1} W_{s_j} [W_{s_{j+1}} - W_{s_j}] \\
&= \sum ( W_{s_j}W_{s_{j+1}} - W_{s_j}^2) \\
&= \sum (W_{s_{j+1}}^2 - W_{s_j}^2 
	- W_{s_{j+1}}^2 + W_{s_j} W_{s_{j+1}})  \tag{add 0} \\
&= \sum \left(  \frac{ W_{s_{j+1}}^2 - W_{s_j}^2 }{ 2 } 
	+ \frac{ W_{s_{j+1}}^2 - W_{s_j}^2 
		- 2W_{s_{j+1}}^2 + 2 W_{s_j} W_{s_{j+1}} }
		{ 2 }
	\right) \\
&= \sum \left(  \frac{ W_{s_{j+1}}^2 - W_{s_j}^2 }{ 2 } \right)
	+  \sum \frac{ - (W_{s_{j+1}}^2 - W_{s_j}^2)^2 } { 2 } \\
&= - \sum \frac{ W_{s_{j+1}}^2 - W_{s_j}^2 }{ 2 } 
	- \frac{1}{2} \sum (W_{s_{j+1}} - W_{s_j})^2
\end{align*}
Notice the term on the right is the jump in Brownian motion. The jump squared is what? That is just the difference $s_{j+1} - s_j$. Recall that the increment in Brownian motion $W_t - W_s\sim \mathcal{N}(0, t-s)$. The increment squared is a variance term, so it will come out to $\sum s_{j+1} - s_j = t$ since we're integrating $\int_0^t$. 

What about the term on the left side? This is a summation that clearly gives $\frac{1}{2} W_t^2.$ The term $ W_{s_{j+1}}^2 - W_{s_j}^2 $ is not the difference squared; it's the squared difference. When you add them all together, you get the final value squared $W_t^2$. 
\[ \therefore\quad 
	\boxed{ \int_0^t W_s \d W_s 
	= \frac{1}{2} W_t^2 - \frac{t}{2} }. \]
\end{example}

