\chapter{Syllabus}

\section{Lec. 1: Syllabus \& Logistics}
Prof will go over the expecations: expectations in terms of time commitment, workload, etc. We'll also talk about what you'll get out of this course.

Prof has taught this class twice before, so this is the third iteration.
\begin{mybox}{green}{Important shorthands I'll be using:}
	\begin{itemize}
	\item Prof $\equiv$ the professor, Tony Dear
	\item h $\equiv$ hour(s); min $\equiv$ minute(s)
	\end{itemize}
\end{mybox}


\paragraph*{Plan for Today}
\begin{itemize}
\item Syllabus and logistics
\item Definition, foundations, and modern capabilities of AI
\item Properties of \textbf{task environments}
\item Structure and types of \textbf{intelligent agents}
\end{itemize}

Reading: Today's material will correspond to chapters 1 and 2 of the textbook.

\subsection*{Course Expectations}
\begin{itemize}
\item MS-level (4k) CS course
	\begin{itemize}
	\item
		Your peers: Mostly CS undergrads, CS grads, and SEAS grad
	\item
		Some taking first 4k course, others first CS course
	\item
		Must be able to learn independently, keep up with course reading
	\end{itemize}

\item \textbf{Coursework}:
	\begin{itemize}
	\item
		Both \textbf{programming} AND \textbf{quantitative analysis (math: probability theory, maybe some linear algebra)}
	\end{itemize}

\item \textbf{Attendance}:
	\begin{itemize}
		\item not required, but try your best to attend live
		\item recordings uploaded by CVN within 24 hours of each lecture
	\end{itemize}
\item \textbf{Half-semester course / Immersive Course}:
	\begin{itemize}
	\item
		covering \textbf{same material as full semester twice as fast} as usual
	\item
		expect \textbf{workload equivalent to 2 regular courses}

	\item
		University requires 18 h total weekly (6 h in class, 12 h outside of class) for immersive courses
	\end{itemize}

\item \textbf{Enrollment}
	\begin{itemize}
	\item
		Section 001 (Fall A) closes Friday 9/11 and section 002 (Fall B) closes Friday 9/18
	\item \textbf{Auditing}: Students interested in auditing and who do not intend to enroll in Fall B may petition to audit by completing this form: \url{https://forms.gle/x8xJugaNa4JcKE4G6}. \emph{If flexible, you're encouraged to audit Fall B --fewer students, better experience for everyone}
	\end{itemize}
\end{itemize}

\subsection*{Grade Breakdown}

\paragraph*{Assessments:} Quizzes (25\%), Homework (40\%), 	Final Exam (35\%)

\paragraph*{Grading: } Grade determined based on two scales:
\begin{enumerate}
\item a fixed standard scale (A is 90\%+, B is 80\%+, etc.) as well as
\item a curved scale relative to all other students,
\end{enumerate}
where the average of the latter is around a B to B+. We will calculate your grade according to both scales and give you the higher of the two. It is thus possible
for everyone to receive an A in the course, but impossible for everyone to fail the course.

\subsubsection*{Next page includes:}
\begin{itemize}
	\item Course description
	\item Objectives
	\item Prerequisites
	\item Topics covered
	\item Reading Assignments
	\item Assessments and Grading
	\item Lectures and Quizzes
	\item Homework
	\item Late Submissions and Drops
	\item Regrading
	\item Academic Conduct and Integrity
\end{itemize}

\section{Lec. 1: Jan 13}



theorem:
$\phi$ is monotone increasing
\[ \pr(|x| \geq a)  = \frac{\expec[ \phi(|x|)]}{ \phi(a)}.\]

Take $\phi(x) = x^2$.
\[
\begin{aligned}
\implies Y = |x - \expec[x]| \\
\implies \pr(|x - \expec[x]| \geq a) \leq \frac{ \expec(|x - \expec[x]|^2) }{ a^2 }
\end{aligned}
\]


Why si this useful? It means that if you knwo how to control the variance, then you know how to control the probability. In the more general case, $\phi$ might be the third (or other higher order) moments.

\begin{proof}
$\pr(|x| \geq a) = \pr( \phi(|x|) \geq \phi(a) )$. Then Markov's Inequality.
\end{proof}


Chebyshev Inequality is one of the fundamental inequalities you should have seen. You should also be familiar with moment generating functions.

Another one you should know: Jensen's Inequality.

Jensen's Inequality (Theorem): Let $f(x)$ be convex. Then, $\expec[f(x)] \geq f(\expec[x])$.

Another one that is important is cauchy-schwarz .

Cauchy Schwarz Inequality (Theorem): Suppose you have two random variables, $X$ and $Y$ s.t. $\expec[X^2] < \infty$ and $\expec[Y^2] < \infty$.
\[ \implies \expec[XY]^2 \leq \expec[X^2] \expec[Y^2]. \]

\begin{proof}
$\forall a, b \in \mathbb{R}$ define $Z = aX - bY$. You can then show that
\[\begin{aligned}
\expec[Z^2] &= \expec[(aX - bY)^2] = a^2 \expec[X^2] - 2ab \expec[XY] + b^2 \expec[Y^2] \geq 0 . \\
&\implies \;\; (2b \expec[XY])^2 - 4 \expec[X^2] \cdot b^2 \expec[Y^2] \leq 0 \\
&\implies \;\; (\expec[XY])^2 \leq \expec[X^2] \expec[Y^2]
\end{aligned}\]
\end{proof}


\subsection*{7. Characteristic Function}
We're concerned with the characteristic fn of random variables, function spaces, or distributions. It's all the same stuff. It doesn't matter.

Let $X$ be a R.V. on ($\Omega, F, \pr$). Given $\phi(t) := \expec[e^{itX}] \forall t\in \mathbb{R}$.

\begin{note}
This is called a fourier transform. It looks similar to the moment generating function,
$M_X(t) \equiv \expec[e^{tX}], \;\; t\in\mathbb{R}$.
\end{note}


$\phi(t) = \int_{-\infty}^{\infty} e^{itx} f(x) dx, \;\;\; f(x)dx := dF(x)$

Example 1.

$X\sim \text{Unif}(a, b)$.

$\phi_X(t) = \expec[e^{itX}] = \int\limits_a^b$


Example 2.

$X \sim \mathcal{N}(0, 1)$.

$\phi_X(t) = \expec[e^{itX}] = \int\limits_{-\infty}^\infty e^{itx} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx$

$= e^{\frac{-1}{2}t^2}$


\textbf{THm}:
\begin{gather}
\phi(0) = 1 \\
|\phi(t)| \leq 1, \forall t\in \mathbb{R}
\end{gather}
\begin{proof}
\[\begin{aligned}
|\phi(t)| = |\int e^{itX} dF| \leq int |e^{itX}| dF \leq 1.
\end{aligned}\]
\end{proof}


\textbf{Thm}: Let $\{ x_k\}_{k=1}^n$ be independent. Let $z = \sum_k x_k$.

How do I find a distribution of $z$? We do a convolution.

$\phi_z(t) = \phi_{x_1} + \ldots + \phi_{x_n}$.  By performing an inverse Fourier transform of the RHS, I can find the characteristic function. The "convolution" will give me the distribution. "We don't need to prove this."


\subsubsection*{Thm} Let $x$ (from above) be s.t. $\expec[x^n] < \infty$. Then, $\forall k\leq n,$ $\phi^{(k)}(t) = i^k \int x^k e^{itx} dF(x)$

$\implies \phi^{(k)}(0) = i^k \int x^k dF(x)  = i^k \expec[x^k]$

$\implies \expec[x^k] = i^{-k} \phi^{(k)}(0)$. The superscript notation denotes the $k$th derivative.


\subsection{Law of Large Numbers (LLN)}

\subsubsection*{Bernoulli's Weak LLN (Thm)}

Why is it weak? We'll explore this. It has to do with weak convergence.

This theorem involves looking at a sequence of i.i.d. random variables.  Let $\{ x_n\}_{n\in N}$ be  a seq of i.i.d. R.V.s with $\sigma^2 = \text{Var}(x_n)$

Define $S_n = \sum_{k=1}^n x_k$. Then, $\dfrac{S_n}{n} \xrightarrow[]{\pr} \mu := \expec[x_n]$ as $n\to\infty$.

\begin{mybox}{green}{definition of "convergence in probability"}
\[\forall \epsilon > 0 , \;\;\; \lim\limits_{n\to\infty} \pr(|\frac{S_n}{n} - \mu| \geq \epsilon) = 0 \]
\end{mybox}

\begin{proof}
By the Chebyshev Ineq., $\pr\left( |\frac{S_n}{n} - \mu| \geq \epsilon \right)
	\leq \frac{\expec[(\frac{S_n}{n} - \mu)^2] }{ \epsilon^2 }
= \frac{  \frac{1}{n^2} \expec[(S_n - n\mu)^2] }{ \epsilon^2 } $

$= \frac{text{Var} (S_n) }{ n^2\epsilon^2 } = \frac{n\sigma^2}{ n^2\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0$.
\end{proof}

\subsubsection*{Kinchtin Weak LLN (Thm):} Let $\{ X_n\}$ be i.i.d. be R.V. with $\mu := \expec[X_n] < \infty$. Then, $\forall \epsilon,$ $n\to \infty \implies \pr(|\frac{S_n}{n} - \mu| \geq \epsilon) \to 0$.



\begin{note}
There is a final project, and you'll have more information about it throughout the next few weeks.

A homework will come out next week on Monday. There's a link on courseworks to the office hours and the syllabus section.
\end{note}



\section{Reading: Artificial Intelligence Intro (Book ch.1)}
