\chapter{Introduction, Intelligent Agents}

% ---------------------------------------
\section{Probability Theory I [Lec. 0, Jan 11]}
% ---------------------------------------

\subsection{Course Overview}

The course will be non-traditional. It's not going to be your typical course found in a statistics or pure math department. What we'll do is present tools from stochastic analysis that are often useful in research and in industry for modeling physical systems.

The usual treatment of this subject is to go over some theoretical results and then talk about a few applications in finance. What we want to look at is the applications of this field in applied math. For instance, elliptic partial differential eqs, monte carlo methods, etc. This will cover the first few chapters of the textbook.

The goal is to gain an overall intuition for the subject, so we're not going to talk about all of the technical details. This doesn't mean we'll have fallacies in all of our derivations. It just means that we won't prove everything so that we can save time. We'll mostly look at the big picture and the connection to different things. We'll talk about why certain abstract things are actually useful.

Consequently, you'll see a lot of jumps. We'll also review elementary knowledge in this area and computing.

The first few homeworks will be a recap of some probability theory that we'll use. Limiting theorems, random variables, and distributions. The rest of the homework is mostly on projects. We will sometimes have simple derivations, schemes, code implementations of course concepts.


Today won't even be a review. We'll just mention what knowledge you will need.

\subsection{Probability Theory Review}

\begin{enumerate}
	\item probability spaces: ($\Omega, F, \pr$) = sample space, $\sigma$-algebra, probability measure

	A sigma alebra has a few properties (in first few chapters of textbook). "countable union"
	\begin{itemize}
		\item $\phi \in F$
		\item $A\in F \implies A^c\in F$
		\item $\{ A_i\}_{i=1}^\infty \in F \implies \cup A_i \in F$
	\end{itemize}

	A probability measure is a function that maps between 0 and 1. $\pr: f\to [0,1]$.
	\begin{itemize}
		\item $E_1 \subseteq E_2 \implies \pr(E_1) \leq \pr(E_2)$
		\item Boole's Inequality: $\pr(\bigcup\limits_{i=1}^\infty E_1)  \leq \sum\limits_i \pr(E_i)$ 
		\item Inclusion-Exclusion: $\pr(E_1\cup E_2) = \pr(E_1) + \pr(E_2) - \pr(E_1 \cap E_2)$
	\end{itemize}
	
	\item  Conditional Probability 
	\begin{itemize}
		\item independence def.
		\item conditional prob. def.,  Bayes' Thm
		\item Law of total prob.: Let $\{E_i\}$ be pairwise disjoint s.t. $\bigcup_i E_i = \Omega$ and $\pr(E_i) > 0$. Then, $\pr(E) = \sum_i \pr(E|E_i) \pr(E_i) = \sum_i \pr(E\cap E_i)$. 
	\end{itemize}
	
	\item \textbf{Random Variables}.
	
	A random variable is a measurable real-valued function, $X(\omega): \Omega\to \mathbb{R}$.

	Measurable $\equiv \; \forall x, \; \{\omega | X(\omega) \leq x\} \subset F $ 

	Distribution: The probability distribution function, $\pr(X\leq x) = F_X(x)$.  
	If \[\exists f_X(x)\text{ s.t. }F_X(x) = \int\limits_{-\infty}^x f_X(t)dt, \;\; \forall x,\] 
	then $f_X$ is a PDF and  $F_X$ is a CDF.

	Expectation: $\expec[X] = \int\limits_{\Omega} x f_X(t)dt$. Sometimes we write this more simply as 
	\[\expec[X] = \int\limits_\Omega X d\pr = \int\limits_{-\infty}^\infty x f_X(x) dx.\] 

	Thm: $X\geq 0 \implies \expec[X] \geq 0$. 

	\[ \expec[a + bX] = a + b\expec[X]	 \]

	\[ \expec[aX + bY] = a\expec[X] + b\expec[Y] \]

	\[ \{X_i\}_i\text{ independent }\implies \expec[\prod_i X_i] = \prod_i \expec[X_i] \]
\end{enumerate}

Variance: Var($X$) = $\expec[(X - \expec[X])^2]$

\[ 
	\text{Cov}(X, Y) = \expec[(X - \expec[X])(Y - \expec[Y])]
\]

\[
	\text{Corr}(X, Y) = \frac{ \text{Cov}(X, Y)}{ \sqrt{\sigma_X^2 \sigma_Y^2} }	
\]

Also note that $\sigma_X^2 = \expec[X^2] - \expec[X]^2$

\[ \Var(a + bX) = b^2 \Var(X) \]

\[ \Var(X + Y) = \Var(X) + \Var(Y) + 2\text{Cov}(X, Y) \]


% ---------------------------------------
\section{Probability Theory II [Lec. 1, Jan 13]}
% ---------------------------------------

Moment inequalities

Thm Markov's Ineq

If $\expec[X] <\infty$, then 
\[ \pr(|X| \geq a) \leq \frac{\expec[|X|]}{a}, \;\; a\geq 0 . \]

\begin{proof}
	
\end{proof}

theorem:
$\phi$ is monotone increasing
\[ \pr(|x| \geq a)  = \frac{\expec[ \phi(|x|)]}{ \phi(a)}.\]

Take $\phi(x) = x^2$.
\[
\begin{aligned}
\implies Y = |x - \expec[x]| \\
\implies \pr(|x - \expec[x]| \geq a) \leq \frac{ \expec(|x - \expec[x]|^2) }{ a^2 }
\end{aligned}
\]




Why si this useful? It means that if you knwo how to control the variance, then you know how to control the probability. In the more general case, $\phi$ might be the third (or other higher order) moments.

\begin{proof}
$\pr(|x| \geq a) = \pr( \phi(|x|) \geq \phi(a) )$. Then Markov's Inequality.
\end{proof}


Chebyshev Inequality is one of the fundamental inequalities you should have seen. You should also be familiar with moment generating functions.

Another one you should know: Jensen's Inequality.

Jensen's Inequality (Theorem): Let $f(x)$ be convex. Then, $\expec[f(x)] \geq f(\expec[x])$.

Another one that is important is cauchy-schwarz .

Cauchy Schwarz Inequality (Theorem): Suppose you have two random variables, $X$ and $Y$ s.t. $\expec[X^2] < \infty$ and $\expec[Y^2] < \infty$.
\[ \implies \expec[XY]^2 \leq \expec[X^2] \expec[Y^2]. \]

\begin{proof}
$\forall a, b \in \mathbb{R}$ define $Z = aX - bY$. You can then show that
\[\begin{aligned}
\expec[Z^2] &= \expec[(aX - bY)^2] = a^2 \expec[X^2] - 2ab \expec[XY] + b^2 \expec[Y^2] \geq 0 . \\
&\implies \;\; (2b \expec[XY])^2 - 4 \expec[X^2] \cdot b^2 \expec[Y^2] \leq 0 \\
&\implies \;\; (\expec[XY])^2 \leq \expec[X^2] \expec[Y^2]
\end{aligned}\]
\end{proof}


\subsection*{7. Characteristic Function}
We're concerned with the characteristic fn of random variables, function spaces, or distributions. It's all the same stuff. It doesn't matter.

Let $X$ be a R.V. on ($\Omega, F, \pr$). Given $\phi(t) := \expec[e^{itX}] \forall t\in \mathbb{R}$.

\begin{note}
This is called a fourier transform. It looks similar to the moment generating function,
$M_X(t) \equiv \expec[e^{tX}], \;\; t\in\mathbb{R}$.
\end{note}


$\phi(t) = \int_{-\infty}^{\infty} e^{itx} f(x) dx, \;\;\; f(x)dx := dF(x)$

Example 1.

$X\sim \text{Unif}(a, b)$.

$\phi_X(t) = \expec[e^{itX}] = \int\limits_a^b$


Example 2.

$X \sim \mathcal{N}(0, 1)$.

$\phi_X(t) = \expec[e^{itX}] = \int\limits_{-\infty}^\infty e^{itx} \frac{1}{\sqrt{2\pi}} e^{-\frac{x^2}{2}} dx$

$= e^{\frac{-1}{2}t^2}$


\textbf{THm}:
\begin{gather}
\phi(0) = 1 \\
|\phi(t)| \leq 1, \forall t\in \mathbb{R}
\end{gather}
\begin{proof}
\[\begin{aligned}
|\phi(t)| = |\int e^{itX} dF| \leq int |e^{itX}| dF \leq 1.
\end{aligned}\]
\end{proof}


\textbf{Thm}: Let $\{ x_k\}_{k=1}^n$ be independent. Let $z = \sum_k x_k$.

How do I find a distribution of $z$? We do a convolution.

$\phi_z(t) = \phi_{x_1} + \ldots + \phi_{x_n}$.  By performing an inverse Fourier transform of the RHS, I can find the characteristic function. The "convolution" will give me the distribution. "We don't need to prove this."


\subsubsection*{Thm} Let $x$ (from above) be s.t. $\expec[x^n] < \infty$. Then, $\forall k\leq n,$ $\phi^{(k)}(t) = i^k \int x^k e^{itx} dF(x)$

$\implies \phi^{(k)}(0) = i^k \int x^k dF(x)  = i^k \expec[x^k]$

$\implies \expec[x^k] = i^{-k} \phi^{(k)}(0)$. The superscript notation denotes the $k$th derivative.


\subsection{Law of Large Numbers (LLN)}

\subsubsection*{Bernoulli's Weak LLN (Thm)}

Why is it weak? We'll explore this. It has to do with weak convergence.

This theorem involves looking at a sequence of i.i.d. random variables.  Let $\{ x_n\}_{n\in N}$ be  a seq of i.i.d. R.V.s with $\sigma^2 = \text{Var}(x_n)$

Define $S_n = \sum_{k=1}^n x_k$. Then, $\dfrac{S_n}{n} \xrightarrow[]{\pr} \mu := \expec[x_n]$ as $n\to\infty$.

\begin{mybox}{green}{definition of "convergence in probability"}
\[\forall \epsilon > 0 , \;\;\; \lim\limits_{n\to\infty} \pr(|\frac{S_n}{n} - \mu| \geq \epsilon) = 0 \]
\end{mybox}

\begin{proof}
By the Chebyshev Ineq., $\pr\left( |\frac{S_n}{n} - \mu| \geq \epsilon \right)
	\leq \frac{\expec[(\frac{S_n}{n} - \mu)^2] }{ \epsilon^2 }
= \frac{  \frac{1}{n^2} \expec[(S_n - n\mu)^2] }{ \epsilon^2 } $

$= \frac{text{Var} (S_n) }{ n^2\epsilon^2 } = \frac{n\sigma^2}{ n^2\epsilon^2} = \frac{\sigma^2}{n\epsilon^2} \to 0$.
\end{proof}

\subsubsection*{Kinchtin Weak LLN (Thm):} Let $\{ X_n\}$ be i.i.d. be R.V. with $\mu := \expec[X_n] < \infty$. Then, $\forall \epsilon,$ $n\to \infty \implies \pr(|\frac{S_n}{n} - \mu| \geq \epsilon) \to 0$.



\begin{note}
There is a final project, and you'll have more information about it throughout the next few weeks.

A homework will come out next week on Monday. There's a link on courseworks to the office hours and the syllabus section.
\end{note}

\section{[Lec. 2, Jan 20]}



